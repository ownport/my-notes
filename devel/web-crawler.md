# web crawler

## Talking about a crawler

 1. Get a url from a task queu
 2. DNS resolution
 3. Request HTTP Header
 4. Download full content
 5. Store to local file store, database and index
 
Add in scheduling, throttling, status monitoring, scale up by flicking on more servers.

From [Python distributed programming using gevent and redis](http://pydanny-event-notes.readthedocs.org/en/latest/KiwiPycon2011/python_dist_gevent_redis.html), [presentation](http://www.slideshare.net/alexdong/kiwipycon2011-asyncwithgeventredis)

